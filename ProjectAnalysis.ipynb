{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e43960ac-d0c1-4786-8930-5f57c9658d66",
   "metadata": {},
   "source": [
    "# GETTING STARTED WITH KAGGLE COMPETITIONS\n",
    "Author: *Melissa Liao*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ee2d69-523f-4495-9b8a-91860260a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5808c-296f-4899-808b-37b62dab1b63",
   "metadata": {},
   "source": [
    "We will go through my first Kaggle competition project, the `Titanic - Machine Learning from Disaster`. In this project, we are going to follow the following workflow: \n",
    "1. Download and load the data given by the site: `train.csv` and `test.csv`.\n",
    "2. Inspect the data to analyze any relationship between the features.\n",
    "3. Preprocess any discrete, nominal and string data.\n",
    "4. Fit and compare models using cross-validation (using our defined functions).\n",
    "5. Apply hyperparameter tuning using grid search (using our defined functions) to find the best model.\n",
    "6. Retrain best model on data and predict test data.\n",
    "7. Submit predicted results to Kaggle and conclude my interpretations.\n",
    "8. Reflect on this mini-project experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97f532-3ef2-410c-a1f1-fa25d011fadb",
   "metadata": {},
   "source": [
    "## 0. Function definitions\n",
    "\n",
    "We will defined our own functions to make it easier to find the best model and hyperparameters that outputs the best accuracy scores for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8afd7168-ea91-4bc2-8477-b52c5e17c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def get_classifier_cv_score(model, X, y, scoring='accuracy', cv=7):\n",
    "    '''Calculate train and validation scores of classifier (model) using cross-validation\n",
    "        \n",
    "        \n",
    "        model (sklearn classifier): Classifier to train and evaluate\n",
    "        X (numpy.array or pandas.DataFrame): Feature matrix\n",
    "        y (numpy.array or pandas.Series): Target vector\n",
    "        scoring (str): a scoring string accepted by sklearn.metrics.cross_validate()\n",
    "        cv (int): number of cross-validation folds see sklearn.metrics.cross_validate()\n",
    "        \n",
    "        returns: mean training score, mean validation score\n",
    "    \n",
    "    '''\n",
    "    model.fit(X, y)\n",
    "    scores = cross_validate(model, X, y, cv=cv, \n",
    "                            scoring=scoring, \n",
    "                            return_train_score=True)\n",
    "    return scores['train_score'].mean(), scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3244c999-bbde-41e0-a67b-438cd77f17b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grid_search_result(grid_search):\n",
    "    '''Prints summary of best model from GridSearchCV object.\n",
    "    \n",
    "        For the best model of the grid search, print:\n",
    "        - parameters \n",
    "        - cross-validation training score\n",
    "        \n",
    "        scores are printed with 3 decimal places.\n",
    "        grid_search (sklearn GridSearchCV): Fitted GridSearchCV object\n",
    "        returns: None\n",
    "\n",
    "    '''\n",
    "    print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "    print(\"Best cross-validation score: {:.3f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4205d78-d058-4166-926d-61fc31a2c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "\n",
    "def plot_grid_search_results(grid_search):\n",
    "    '''For grids with 2 hyperparameters, create a heatmap plot of test scores\n",
    "        grid_search (sklearn GridSearchCV): Fitted GridSearchCV object\n",
    "        uses mglearn.tools.heatmap() for plotting.\n",
    "        \n",
    "    '''\n",
    "    results = pd.DataFrame(grid_search.cv_results_)\n",
    "    params = sorted(grid_search.param_grid.keys())\n",
    "    assert len(params) == 2, \"We can only plot two parameters.\"\n",
    "    \n",
    "    # second dimension in reshape are rows, needs to be the fast changing parameter\n",
    "    scores = np.array(results.mean_test_score).reshape(len(grid_search.param_grid[params[0]]),\n",
    "                                                      len(grid_search.param_grid[params[1]]))\n",
    "\n",
    "    # plot the mean cross-validation scores\n",
    "    # x-axis needs to be the fast changing parameter\n",
    "    mglearn.tools.heatmap(scores, \n",
    "                          xlabel=params[1], \n",
    "                          xticklabels=grid_search.param_grid[params[1]], \n",
    "                          ylabel=params[0], \n",
    "                          yticklabels=grid_search.param_grid[params[0]],\n",
    "                          cmap=\"viridis\", fmt=\"%0.3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173dd149-60c4-4ad9-aca2-dd3d2e898815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
